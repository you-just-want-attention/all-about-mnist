{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist 다운받을 때 필요\n",
    "!pip install wget\n",
    "\n",
    "# master 브랜치를 가져옴\n",
    "!git clone https://github.com/you-just-want-attention/all-about-mnist.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"./all-about-mnist/utils/\")\n",
    "sys.path.append(\"../../utils/\")\n",
    "from dataset import CalculationDataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Hyper Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "char_arr = [c for c in 'SEP1234567890(){}[]+-*/'] # embedding table\n",
    "char_dict = {i:char for i,char in enumerate(char_arr)}\n",
    "\n",
    "# 모델에 대한 Setting\n",
    "N_CLASS = len(char_arr)\n",
    "N_HIDDEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 데이터셋에 대한 Setting\n",
    "num_digit = 3\n",
    "n_step = int(num_digit + (num_digit - 1) + (num_digit//2)*2 + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 데이터 셋 제너레이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dataset, char_arr, batch_size=32, n_step=30, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_step = n_step\n",
    "        self.char_arr = char_arr\n",
    "        self.num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "        self.n_class = len(self.num_dic)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _, eq_results, _, equations = self.dataset[self.batch_size * index:\n",
    "                                                   self.batch_size * (index+1)]\n",
    "        eq_results = eq_results.astype(int).astype(str)\n",
    "        seq_data = np.stack([equations,eq_results],axis=-1)\n",
    "        input_batch, output_batch, target_batch = self.make_batch(seq_data)\n",
    "        return input_batch, output_batch, target_batch\n",
    "    \n",
    "    def make_batch(self, seq_data):\n",
    "        input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "        for idx, (equation, result) in enumerate(seq_data):\n",
    "            # input은 패딩, target은 한 후 padding 지점 앞 marking\n",
    "            if self.n_step < len(equation):\n",
    "                raise ValueError(\"n_Step이 너무 작습니다. 더 큰값으로 설정해주세요\")\n",
    "            \n",
    "            equation = equation + \"P\" * (self.n_step-len(equation))\n",
    "            result = result + \"E\" + \"P\" * (self.n_step-len(result)-1)\n",
    "\n",
    "            input_data = [self.num_dic[n] for n in equation]\n",
    "            output_data = [self.num_dic[n] for n in ('S' + result[:-1])]\n",
    "            target_data = [self.num_dic[n] for n in result]\n",
    "\n",
    "            input_batch.append(np.eye(self.n_class)[input_data])\n",
    "            output_batch.append(np.eye(self.n_class)[output_data])\n",
    "            # one-hot으로 들어가는 것 아님\n",
    "            target_batch.append(target_data)\n",
    "\n",
    "        # make tensor\n",
    "        return torch.Tensor(input_batch), torch.Tensor(output_batch), torch.LongTensor(target_batch)     \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle:\n",
    "            self.dataset.shuffle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋에 대한 Generator\n",
    "train_set = CalculationDataset('train', digit=num_digit,)\n",
    "traingen = BatchGenerator(train_set, char_arr, BATCH_SIZE, n_step=n_step)\n",
    "\n",
    "# 테스트 데이터셋에 대한 Generator\n",
    "test_set = CalculationDataset('train', digit=num_digit,)\n",
    "testgen = BatchGenerator(test_set, char_arr, BATCH_SIZE, n_step=n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size : 32\n",
      "padded sentence size : 9\n",
      "vocabulary size : 23\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋이 올바른지 확인해보기\n",
    "input_batch, output_batch, target_batch = traingen[0]\n",
    "\n",
    "print(\"data size : {}\".format(len(input_batch)))\n",
    "print(\"padded sentence size : {}\".format(len(input_batch[0])))\n",
    "print(\"vocabulary size : {}\".format(len(input_batch[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_data(data):\n",
    "    \"\"\"\n",
    "    Encoding 된 data를 다시 문자로 바꾸어주는 역할을 함\n",
    "    \"\"\"\n",
    "    if isinstance(data,torch.Tensor):\n",
    "        data = data.detach().numpy()\n",
    "    if np.ndim(data) == 2:\n",
    "        data = np.expand_dims(data,axis=0)\n",
    "    return (\n",
    "    pd.DataFrame(data.argmax(axis=-1))\n",
    "       .applymap(lambda x : char_dict[x])\n",
    "       .apply(lambda x : x.sum(),axis=1)\n",
    "       .apply(lambda x : x.replace(\"S\",\"\"))\n",
    "       .apply(lambda x : x.replace(\"E\",\"\"))        \n",
    "       .apply(lambda x : x.replace(\"P\",\"\"))                \n",
    "       .values\n",
    "       .tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6',\n",
       " '0',\n",
       " '1',\n",
       " '5',\n",
       " '5',\n",
       " '1',\n",
       " '0',\n",
       " '74',\n",
       " '0',\n",
       " '2',\n",
       " '7',\n",
       " '9',\n",
       " '16',\n",
       " '0',\n",
       " '294',\n",
       " '-27',\n",
       " '-2',\n",
       " '9',\n",
       " '3',\n",
       " '-1',\n",
       " '0',\n",
       " '-1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '18',\n",
       " '36',\n",
       " '0',\n",
       " '12',\n",
       " '10',\n",
       " '-3',\n",
       " '-1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_data(output_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8-7/(4)',\n",
       " '1/(9*(6))',\n",
       " '8-(7)-(0)',\n",
       " '0*5+(5)',\n",
       " '7-7+(5)',\n",
       " '4+((4-7))',\n",
       " '(8)/7/2',\n",
       " '(8)*9+2',\n",
       " '0*9/(8)',\n",
       " '4*(3/6)',\n",
       " '4+(0)+(3)',\n",
       " '(1)*1*(9)',\n",
       " '9-(0)+7',\n",
       " '7*9*(0)',\n",
       " '7*6*(7)',\n",
       " '1-4*(7)',\n",
       " '(6/(4-7))',\n",
       " '(9+(1)/4)',\n",
       " '(3)-0/4',\n",
       " '(6-(7))-0',\n",
       " '3/(9-3)',\n",
       " '1-((1))-1',\n",
       " '9-9*(1)',\n",
       " '0+1/((7))',\n",
       " '1-(6)+6',\n",
       " '7*2+(4)',\n",
       " '7*(4)+8',\n",
       " '(0)+(7)/8',\n",
       " '6*2-(0)',\n",
       " '(1)+4+(5)',\n",
       " '5-(1)*(8)',\n",
       " '7-2-(6)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_data(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # PyTorch RNN class implements the Elman(vanilla) RNN\n",
    "        self.enc_cell = nn.RNN(input_size=N_CLASS, \n",
    "                               hidden_size=N_HIDDEN,\n",
    "                               dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=N_CLASS, \n",
    "                               hidden_size=N_HIDDEN,\n",
    "                               dropout=0.5)\n",
    "        self.fc = nn.Linear(N_HIDDEN, N_CLASS)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1)\n",
    "        dec_input = dec_input.transpose(0, 1)\n",
    "\n",
    "        _, enc_states = self.enc_cell(enc_input,\n",
    "                                      enc_hidden)\n",
    "        outputs, _ = self.dec_cell(dec_input,\n",
    "                                   enc_states)\n",
    "\n",
    "        model = self.fc(outputs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksj/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "encoding_cell = nn.RNN(input_size=N_CLASS,\n",
    "                       hidden_size=N_HIDDEN,\n",
    "                       dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:33<00:00, 17.28it/s]\n",
      "  0%|          | 0/572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Epoch :  1 | cost 17.472\n",
      "--------------------------\n",
      "Test------------\n",
      "0th test case : \n",
      "1+((2/4)) ->\n",
      "answer :  1\n",
      "prediction:  11111111111111111111111111111111\n",
      "\n",
      "\n",
      "1th test case : \n",
      "(4*9)-3 ->\n",
      "answer :  33\n",
      "prediction:  \n",
      "\n",
      "\n",
      "2th test case : \n",
      "7-9+((6)) ->\n",
      "answer :  4\n",
      "prediction:  \n",
      "\n",
      "\n",
      "3th test case : \n",
      "7+(4*(9)) ->\n",
      "answer :  43\n",
      "prediction:  \n",
      "\n",
      "\n",
      "4th test case : \n",
      "9+4*(1) ->\n",
      "answer :  13\n",
      "prediction:  \n",
      "\n",
      "\n",
      "5th test case : \n",
      "9/5+(3) ->\n",
      "answer :  4\n",
      "prediction:  \n",
      "\n",
      "\n",
      "6th test case : \n",
      "1*(2)+(7) ->\n",
      "answer :  9\n",
      "prediction:  \n",
      "\n",
      "\n",
      "7th test case : \n",
      "1+0-(9) ->\n",
      "answer :  -8\n",
      "prediction:  \n",
      "\n",
      "\n",
      "8th test case : \n",
      "(6+(8)-0) ->\n",
      "answer :  14\n",
      "prediction:  \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 382/572 [00:22<00:11, 16.69it/s]"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epoch = 100 # Epoch 수\n",
    "for epoch in range(num_epoch):\n",
    "    for step in tqdm(range(len(traingen))):\n",
    "        input_batch, output_batch, target_batch = traingen[step]\n",
    "        \n",
    "        # hidden value 초기화\n",
    "        hidden = torch.zeros(1, BATCH_SIZE, N_HIDDEN)\n",
    "        # 이전 interation에서 축적 됐을 수도 있으니 값을 초기화.\n",
    "        optimizer.zero_grad()\n",
    "        # 학습 데이터 모델에 전달(input_batch, hidden, output_batch)\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        output = output.transpose(0, 1)\n",
    "        loss = 0\n",
    "        # 결과와 정답 .... and element 각각 비교\n",
    "        for i in range(0, len(target_batch)):\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        # propagages the loss value back through the network. \n",
    "        loss.backward()\n",
    "        # 미분한거 업데이트 하는 function.\n",
    "        optimizer.step()\n",
    "    print(\"--------------------------\")\n",
    "    print('Epoch : {:2d} | cost {:.3f}'.format(epoch+1,loss))\n",
    "    print(\"--------------------------\")\n",
    "    # Sample Dataset 평가\n",
    "    print(\"Test------------\")\n",
    "    input_batch, output_batch, target_batch = testgen[0]\n",
    "\n",
    "    hidden = torch.zeros(1, len(input_batch), N_HIDDEN) \n",
    "    pred_batch = model(input_batch, hidden, output_batch)\n",
    "\n",
    "    decoded_input_batch = decode_data(input_batch)\n",
    "    decoded_output_batch = decode_data(output_batch)\n",
    "    decoded_pred_batch = decode_data(pred_batch)\n",
    "\n",
    "    for idx, (input_str, answer, prediction) in enumerate(\n",
    "        zip(decoded_input_batch, decoded_output_batch,decoded_pred_batch)):\n",
    "        print(\"{}th test case : \".format(idx))\n",
    "        print(input_str,\"->\")\n",
    "        print(\"answer : \",answer)\n",
    "        print(\"prediction: \",prediction)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
