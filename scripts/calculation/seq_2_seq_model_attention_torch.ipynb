{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence to sequence model, attention mechanism\n",
    "\n",
    "paper : [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "- Submitted on 1 Sep 2014 (v1), last revised 19 May 2016 (this version, v7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as before [S2S model](https://github.com/you-just-want-attention/all-about-mnist/blob/master/scripts/calculation/seq_2_seq_model_torch.ipynb)\n",
    "# Digit recognition이 다 되었다고 가정하고 시작\n",
    "\n",
    "the basic code was the project of translation from french to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that shows ending of encoding input\n",
    "\n",
    "char_arr = [c for c in 'SEP.1234567890(){}[]+-*/']\n",
    "word_dic = {i: n for i, n in enumerate(char_arr)}\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "n_class = len(num_dic)\n",
    "seq_data = [['P9-4-(3*5+9)', 'S-19', '-19E' ],\n",
    "            ['0*(3*3-0)-((0))+4', '4'],\n",
    "            ['(9)-5-(4*(5*5))', '-96'],\n",
    "            ['5*(0-(4/1+9)*2-(1-3))*(1)', '-120.0'],\n",
    "            ['5-0/4/(1+9)+2-1', '6.0'],\n",
    "            ['(5-(0)-(4+(1-9)*2)/2)', '9.0']]\n",
    "input_batch = len(seq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Parameter 세팅하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention seq2seq parameter\n",
    "n_hidden=128\n",
    "N_STEP = 26\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_STEP에 대해서는 고민을 좀 했지만... 우리 데이터의 maximum length가 25이고 거기다 symbol까지 더하면 26이므로 그 수치로 잡음..ㅎ.ㅎㅎㅎ\n",
    "\n",
    "yeah this is deep learing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention](https://i.stack.imgur.com/vo6se.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = context*output\\\\\n",
    "attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "output = \\tanh(w * (attn * context) + b * output)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentence):\n",
    "    \n",
    "    input_batch = [np.eye(n_class)[[num_dic[n] for n in sentence[0]]]]\n",
    "    output_batch = [np.eye(n_class)[[num_dic[n] for n in sentence[1]]]]\n",
    "    target_batch = [[num_dic[n] for n in sentence[2]]]\n",
    "\n",
    "    # make tensor\n",
    "    return torch.Tensor(input_batch), torch.Tensor(output_batch), torch.LongTensor(target_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "\n",
    "        # Linear for attention, TODO : attention, out 선형 함수 합쳐야 함.\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
    "\n",
    "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
    "        # enc_input: [batch_size(=data_size), MAXLEN, N_HIDDEN] -> [MAXLEN, batch_size, N_HIDDEN]        \n",
    "        enc_inputs = enc_inputs.transpose(0, 1)\n",
    "        dec_inputs = dec_inputs.transpose(0, 1)\n",
    "\n",
    "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "\n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        model = torch.empty([n_step, 1, n_class])\n",
    "\n",
    "        for i in range(n_step):  # each time step\n",
    "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "\n",
    "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
    "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
    "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
    "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
    "\n",
    "        # make model shape [n_step, n_class]\n",
    "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
    "\n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "\n",
    "        # Normalize scores to weights in range 0 to 1\n",
    "        return F.softmax(attn_scores).view(1, 1, -1)\n",
    "\n",
    "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
    "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', '9', '-', '4', '-', '(', '3', '*', '5', '+', '9', ')']\n"
     ]
    }
   ],
   "source": [
    "input_batch, output_batch, target_batch = make_batch(seq_data[0])\n",
    "hidden = torch.zeros(1, 1, n_hidden)\n",
    "\n",
    "model = Attention()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 cost = 0.000480\n",
      "Epoch: 0800 cost = 0.000157\n",
      "Epoch: 1200 cost = 0.000079\n",
      "Epoch: 1600 cost = 0.000047\n",
      "Epoch: 2000 cost = 0.000031\n",
      "Epoch: 2400 cost = 0.000021\n",
      "Epoch: 2800 cost = 0.000016\n",
      "Epoch: 3200 cost = 0.000012\n",
      "Epoch: 3600 cost = 0.000009\n",
      "Epoch: 4000 cost = 0.000007\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(4000):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(input_batch, hidden, output_batch)\n",
    "\n",
    "    loss = criterion(output, target_batch.squeeze(0))\n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "predict, trained_attn = model(input_batch, hidden, output_batch)\n",
    "predict = predict.data.max(1, keepdim=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21,  4, 12,  1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19E -> ['-', '1', '9', 'E']\n"
     ]
    }
   ],
   "source": [
    "print(seq_data[0][2], '->', [word_dic[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-7414a88e2496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#그리기\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(trained_attn, cmap='viridis')\n",
    "ax.set_xticklabels([''] + seq_data[0][0], fontdict={'fontsize': 14})\n",
    "ax.set_yticklabels([''] + seq_data[0][2], fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
