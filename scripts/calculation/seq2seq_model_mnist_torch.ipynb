{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq-2-seq-model-torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "uk647Cr82f5P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# sequence to sequence model with MNIST\n",
        "\n",
        "paper : [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "5_Xm1E_kf5FQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "dtype = torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DK0KE3aqWB8j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data encoding이 다 되었다고 가정하고 시작\n",
        "\n",
        "the basic code was the project of translation model from english to french"
      ]
    },
    {
      "metadata": {
        "id": "8OdcQkoekb0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "char_arr = [c for c in 'SEP.1234567890(){}[]+-*/']\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "\n",
        "seq_data = [['9-4-(3*5+9)', '-19'], ['0*(3*3-0)-((0))+4', '4'], ['(9)-5-(4*(5*5))', '-96'], ['5*(0-(4/1+9)*2-(1-3))*(1)', '-120.0'], ['5-0/4/(1+9)+2-1', '6.0'], ['5-(0)-(4+(1-9))', '9.0']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "haGsRN-QpxVC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1) Parameter 세팅하기"
      ]
    },
    {
      "metadata": {
        "id": "WrwZ3ud_kbGV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Seq2Seq Parameter\n",
        "n_step = 25 # maxlen+1을 의미함\n",
        "n_hidden = 128\n",
        "n_class = len(num_dic)\n",
        "batch_size = len(seq_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fRqEEFIZ3TGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2) Preprocess the data"
      ]
    },
    {
      "metadata": {
        "id": "A5AvrDgukZdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_batch(seq_data):\n",
        "    input_batch, output_batch, target_batch = [], [], []\n",
        "\n",
        "    for idx, seq in enumerate(seq_data):\n",
        "      \n",
        "      # input은 패딩, target은 한 후 padding 지점 앞 marking\n",
        "      for i in range(len(seq)):\n",
        "        seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
        "#       print(seq)\n",
        "      start_output=seq[1].index('P')\n",
        "      seq[1] = seq[1][:start_output] + 'E' + seq[1][(start_output+1):]\n",
        "\n",
        "      input = [num_dic[n] for n in seq[0]]\n",
        "      output = [num_dic[n] for n in ('S' + seq[1][:-1])]\n",
        "      target = [num_dic[n] for n in seq[1]]\n",
        "\n",
        "      #예시 input, output, target 프린트\n",
        "      if idx==1: print(\"calculation: {}\\ninput : {}\\noutput :{}\\ntarget : {}\".format(seq,input, output, target))\n",
        "\n",
        "\n",
        "      input_batch.append(np.eye(n_class)[input])\n",
        "      output_batch.append(np.eye(n_class)[output])\n",
        "      # one-hot으로 들어가는 것 아님\n",
        "      target_batch.append(target)\n",
        "\n",
        "    # make tensor\n",
        "    return Variable(torch.Tensor(input_batch)), Variable(torch.Tensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F1mH9uvwqDYR",
        "colab_type": "code",
        "outputId": "6cf68497-c20d-4d30-b5cf-97daaa57ca62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "input_batch, output_batch, target_batch = make_batch(seq_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calculation: ['0*(3*3-0)-((0))+4PPPPPPPP', '4EPPPPPPPPPPPPPPPPPPPPPPP']\n",
            "input : [13, 22, 14, 6, 22, 6, 21, 13, 15, 21, 14, 14, 13, 15, 15, 20, 7, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "output :[0, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "target : [7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0gSlqkXmTQZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9904cf8e-0f97-4ea2-9bf3-099bec2c48fb"
      },
      "cell_type": "code",
      "source": [
        "len(input_batch[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "K0naduPUTdAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4319888-cab7-4f42-af73-c7513bd33f11"
      },
      "cell_type": "code",
      "source": [
        "len(output_batch[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "j2i8sL7qMLTZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "S: Symbol that shows starting of decoding input\n",
        "\n",
        "E: Symbol that shows starting of decoding output\n",
        "\n",
        "P: Symbol that will fill in blank sequence if current batch data size is short than time steps "
      ]
    },
    {
      "metadata": {
        "id": "BvSg0x_8T5Uk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "대략적인 data size에 대한 감을 익히기 위한 안내"
      ]
    },
    {
      "metadata": {
        "id": "AL7TdsO3q17F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- len(input_batch) : data_pair_size\n",
        "\n",
        "- len(input_batch[0]) : n_step, number of step\n",
        "\n",
        "- len(input_batch[0][0]) : |vocab|"
      ]
    },
    {
      "metadata": {
        "id": "yARDe3pfmV0d",
        "colab_type": "code",
        "outputId": "32cecff5-0c29-4ee6-ddc8-926c352f9103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1870
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"INPUT BATCH - {}, \\nINPUT BATCH LENGTH - {}\".format(input_batch, len(input_batch)))\n",
        "print(\"OUTPUT BATCH - {}, \\nOUTPUT BATCH LENGTH - {}\".format(output_batch, len(output_batch)))\n",
        "print(\"TARGET BATCH - {}, \\nOUTPUT BATCH LENGTH - {}\".format(target_batch, len(target_batch)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT BATCH - tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]]]), \n",
            "INPUT BATCH LENGTH - 6\n",
            "OUTPUT BATCH - tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.]]]), \n",
            "OUTPUT BATCH LENGTH - 6\n",
            "TARGET BATCH - tensor([[21,  4, 12,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2],\n",
            "        [ 7,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2],\n",
            "        [21, 12,  9,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2],\n",
            "        [21,  4,  5, 13,  3, 13,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2],\n",
            "        [ 9,  3, 13,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2],\n",
            "        [12,  3, 13,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2]]), \n",
            "OUTPUT BATCH LENGTH - 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8-1fhxGbUK0N",
        "colab_type": "code",
        "outputId": "fcf9b35e-c4d1-443e-a098-6c6e0a3f327a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"data size : {}\".format(len(input_batch)))\n",
        "print(\"padded sentence size : {}\".format(len(input_batch[0])))\n",
        "print(\"vocabulary size : {}\".format(len(input_batch[0][0])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data size : 6\n",
            "padded sentence size : 25\n",
            "vocabulary size : 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rs1A0qGXqdQa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3) Model 구성하기"
      ]
    },
    {
      "metadata": {
        "id": "OOlLkhNe7hmO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.fc = nn.Linear(n_hidden, n_class)\n",
        "\n",
        "    def forward(self, enc_input, enc_hidden, dec_input):\n",
        "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_hidden]\n",
        "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_hidden]\n",
        "\n",
        "        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
        "        # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
        "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
        "\n",
        "        model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n",
        "        return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pq8-ezqoqBOO",
        "colab_type": "code",
        "outputId": "b83821c8-245a-49ab-ddee-ea39a9d78753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "model = Seq2Seq()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FvM33gja7rqg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4) 학습 시키기"
      ]
    },
    {
      "metadata": {
        "id": "GPIcmEWx7pLS",
        "colab_type": "code",
        "outputId": "066028ab-3e66-49de-ff14-c4d029b92026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(5000):\n",
        "    hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_batch, hidden, output_batch)\n",
        "    output = output.transpose(0, 1)\n",
        "    loss = 0\n",
        "    for i in range(0, len(target_batch)):\n",
        "        loss += criterion(output[i], target_batch[i])\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000 cost = 0.359929\n",
            "Epoch: 2000 cost = 0.234778\n",
            "Epoch: 3000 cost = 0.003447\n",
            "Epoch: 4000 cost = 0.000744\n",
            "Epoch: 5000 cost = 0.000318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MA7HiRDn7vXl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5) Test하기"
      ]
    },
    {
      "metadata": {
        "id": "A6dL7hLk7wpX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(word):\n",
        "    input_batch, output_batch, _ = make_batch([[word, 'P' * len(word)]])\n",
        "\n",
        "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "    hidden = Variable(torch.zeros(1, 1, n_hidden))\n",
        "    output = model(input_batch, hidden, output_batch)\n",
        "    # output : [max_len+1, batch_size(=1), n_class]\n",
        "\n",
        "    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
        "    decoded = [char_arr[i] for i in predict]\n",
        "    end = decoded.index('E') if 'E' in decoded else decoded.index('P')\n",
        "      \n",
        "     \n",
        "    translated = ''.join(decoded[:end])\n",
        "\n",
        "    return translated.replace('P', '')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tt0IKL5GtzUs",
        "colab_type": "code",
        "outputId": "3beffd0c-1b6d-455a-f548-682321f97ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print('test')\n",
        "print('9-4-(3*5+9) ->', translate('9-4-(3*5+9)'))\n",
        "print('0*(3*3-0)-((0))+4 ->', translate('0*(3*3-0)-((0))+4'))\n",
        "print('(9)-5-(4*(5*5)) ->', translate('(9)-5-(4*(5*5))'))\n",
        "print('5*(0-(4/1+9)*2-(1-3))*(1) ->', translate('5*(0-(4/1+9)*2-(1-3))*(1)'))\n",
        "print('5-0/4/(1+9)+2-1 ->', translate('5-0/4/(1+9)+2-1'))\n",
        "print('5-(0)-(4+(1-9)) ->', translate('5-(0)-(4+(1-9))'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "9-4-(3*5+9) -> -19\n",
            "0*(3*3-0)-((0))+4 -> 4.\n",
            "(9)-5-(4*(5*5)) -> -96\n",
            "5*(0-(4/1+9)*2-(1-3))*(1) -> -12\n",
            "5-0/4/(1+9)+2-1 -> 6.\n",
            "5-(0)-(4+(1-9)) -> 9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "47T4v4Z384fS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d89c4cf-60ac-4b4a-a630-c9a769574df6"
      },
      "cell_type": "code",
      "source": [
        "### 정답\n",
        "import re\n",
        "[re.sub('[EP]', '',\n",
        "        i[1]) for i in seq_data]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-19', '4', '-96', '-120.0', '6.0', '9.0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}